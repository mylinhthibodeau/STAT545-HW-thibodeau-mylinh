---
title: "stat547-hw06-thibodeau-mylinh"
author: "My Linh Thibodeau"
date: '2017-11-05'
output: github_document
always_allow_html: yes
---

```{r message=FALSE}
#suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(tidyverse))
suppressWarnings(library(tidyverse))
knitr::opts_chunk$set(fig.width=13, fig.height=8)
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(options(knitr.table.format = "markdown"))
library(readr)
suppressWarnings(suppressMessages(library(forcats)))
library(purrr)
library(repurrrsive)
library(tibble)
library(stringr)
library(devtools)
library(listviewer)
library(jsonlite)
#install.packages("ontologyIndex")
library(ontologyIndex)
#install.packages("ontologySimilarity")
library(ontologySimilarity)
#install.packages("ontologyPlot") 
#install.packages("Rgraphviz")
#library(ontologyPlot) # does not work yet with the most recent version of R though
```

I installed these new packages of the ontologyX suite [here](https://cran.r-project.org/web/packages/ontologyIndex/vignettes/intro-to-ontologyX.html), which offer tools to explore ontology data. 

We can use the getNamespaceExports() to see what are the functions of these packages !
```{r}
getNamespaceExports("ontologyIndex") 
getNamespaceExports("ontologySimilarity") 
```


First step, get the Human Disease Ontology as explained [here](https://github.com/DiseaseOntology/HumanDiseaseOntology/blob/master/src/ontology/README-editors.md), I had to clone the repository (which I called HumanDiseaseOntology_git, as suggested by the github README document).
```{r}
get_relation_names("HumanDiseaseOntology_git/src/ontology/HumanDO.obo")
HumanDO <- get_ontology("HumanDiseaseOntology_git/src/ontology/HumanDO.obo", propagate_relationships=c("is_a", "part_of"))
#View(HumanDO)
```

I have downloaded a basic version of the Gene Ontology (GO) at [here](http://www.geneontology.org/page/download-ontology) and I will now look into what are the relationships between the GO terms.
```{r}
get_relation_names("GO/go-basic.obo")
GO <- get_ontology("GO/go-basic.obo")
#View(GO)
```

Interestingly, the ontologyIndex package does come with an R version of HPO (Human Phenotype Ontology) and GO (Gene Ontology), which you can load as follow.
```{r}
data(hpo)
#View(hpo)
data(go)
#View(go)
```
*Note.* I will try not use the R lists of the ontologyIndex, because I am trying to learn how to read and manipulate the OBO format files directly. 

# Homework instructions

**Pick (at least) two of the six (numbered) topics below and do one of the exercise prompts listed, or something comparable using your dataset of choice.**

The two tasks I picked are the following:

1. Character data
5. Work with a list

***

# 1. Character data

## Let's take a peak at the data 

For the first task, we will be exploring some character data, and we will use ontology terms for this. This examples is modelled on Daniel Greene's work [here](https://cran.r-project.org/web/packages/ontologySimilarity/vignettes/ontologySimilarity-introduction.html). 

However, I think it would be useful to try and understand the data of HumanDO a bit better first. Here are some key concepts about HumanDO:

* As opposed to the GO dataset above, HumanDO only has one type of relationship and it is "is_a"
* It is a large list of 6 elements
* These 6 elements are:
1. id: specific DOID (Disease Ontology ID) identifier
2. name: specific term attached to the identifier (e.g. angiosarcoma)
3. parents: an ontology goes from general terms (parents) to more specific terms (children)
4. children: on parent can have zero children (if it is a unique term), or many children (if it is a general term which can be further divided into more specific terms)
5. ancestors: this list keep an aggregate list of all the more general terms that preceded a term (all the parents, grand-parents, great grand-parents "terms", etc.)
6. obsolete: this is a boolean list, which includes all the terms/DOID identifiers that were once in the HumanOD ontology: most of these are currently valid ("TRUE"), but some are not in use anymore ("FALSE")

Let us look at an example. Let's look at the diseases that contain the word "encephalitis":
```{r}
head(HumanDO$name[grep(x=HumanDO$name, pattern = "encephalitis")])
```
Note. Use of grep in ontology data also from Daniel Greene's work [here](https://cran.r-project.org/web/packages/ontologyIndex/vignettes/intro-to-ontologyX.html)


Let's look at the diseases that contain the word "Japanese":
```{r}
HumanDO$name[grep(x=HumanDO$name, pattern = "Japanese")]
```

We note that DOID:0050050 is associated with the disease "Japanese spotted fever". Let's look at the ancestor of this term.

```{r}
get_term_property(ontology=HumanDO, property = "ancestors", term = "DOID:0050050",as_names=TRUE)
```
Note. So this is pretty intuitive when we think about it: some terms like "disease" are very general, and when the ontology tree gains more granularity, then the addition of characteristics such as "bacterial", "infectious" and "spotted fever" lead to the creation of a specific ontology disease identifier:diagnosis (DOID:0050050 = Japanese spotted fever).

***

We will use the HumanDO data and set a seed.
```{r}
set.seed(1)
```

Then, we will use the `descendants_IC()` function to calculate information content of terms based on frequency with which it is an ancestor of other terms.
```{r}
information_content <- descendants_IC(HumanDO)
```

Then, we generate 5 random sets of 8 terms.
```{r}
term_sets <- replicate(simplify=FALSE, n=5, expr=minimal_set(HumanDO, sample(HumanDO$id, size=8)))
term_sets 
```

Note that the term_sets variable is a small nested list of characters items: there are 5 lists, each of which contains one list of 8 items, as exemplified here:
```{r}
str(term_sets)
```

In genomics, it can be helpful to compare sets of terms and determine how much similarity is shared between datasets (here, we have 5 lists, or 5 "mini datasets"). We can use the `get_sim_grid()` function to produce a similarity matrix and verify if any dataset is highly similar to another one.
```{r}
similarity_matrix <- get_sim_grid(ontology= HumanDO, term_sets = term_sets)
# similarity_matrix %>% kable(format = "markdown", align="c")
similarity_matrix
```
Note. From the top-left corner to the bottom-right corner, the similarity score is always 1 because we are comparing respectively list 1 with list 1, list 2 with list 2, etc. 

***

## Let's manipulate some strings now

I will be completed the tasks of this R for Data Science tutorial [here](http://r4ds.had.co.nz/strings.html), as suggested in the homework 6 instructions.

### String basics

```{r}
string1 <- "anemia"
string2 <- 'I am looking for the word "anemia" in a list' # Using double quotes inside simple quotes

```

Using one double quote or single quote
```{r}
double_quote <- "\""
double_quote2 <- '"'
single_quote <- '\'' 
single_quote2 <- "'"
```

```{r}
double_quote
double_quote2
single_quote
single_quote2
```
Note. In the tutorial, it seems as if the backslash bar is not printed, but in the example above, it does get printed. It seems that only the use of single quotes provides us with the expected result, so I will keep this format in the future. 

Let's try to print a backslash. These all produced an error message:

* "\"
* "\\"
* "\ "

```{r}
"\ '"
```
Note. This simply does not print a backslash.

As shown in block028 of the character data stat545 website [here](http://stat545.com/block028_character-data.html), using the function cat instead of print allow us to print a backslash (or "escape")
```{r}
cat("Here is a backslash: \\ ")
```

There can be some printed representation of a string that is not the same than the string itself, and using the `writeLines()` function allows us to see the raw content:
```{r}
x <- c("one \"", "two \\")
x
writeLines(x)
```

Other handy special characters are `"\n"` (newline) and `"\t"`
```{r}
cat("We can use \n to print a new line \n\n")
cat("While \t inserts a tab")
```

Some strings actually represent non-English characters in all coding platforms, for example:
```{r}
x <- "\u00b5"
x
```

Strings can be put into a charactor vector
```{r}
c("anemia", "low iron", "pallor")
```

### String length

How many character in this string? Use str_length
```{r}
str_length(c("anemia", "low iron", "pallor"))
```
Note. White spaces count as characters.

### Combining strings

Use `str_c()`
```{r}
str_c("anemia", "low iron", "pallor")
```
Note. As mentioned, white spaces count as characters, so the white space in "low iron" is preserved here.

You can also specify the separator:
```{r}
str_c("anemia", "low iron", "pallor", sep=" = ")
```

You can specify what the missing values "NA" can be replaced with:
```{r}
x <- c("anemia",  NA)
str_c("--> ", x , " <--")
str_c("--> ", str_replace_na(x), " <--")
```

In the example, `str_c()` is vectorized, and it makes the shorter vectors the same length as the longest vector:

```{r}
str_c("prefix-", c("a", "b", "c"), "", "-suffix")
```
Note. Objects of length zero (e.g. "" above) are dropped.

This can be used with an if statement.
```{r}
cancer_type <- "breast cancer"
time_since_diagnosis <- "2 years"
day_of_diagnosis <- FALSE

str_c(
  "It has been ", time_since_diagnosis, " since your diagnosis of ", cancer_type, 
  if (day_of_diagnosis) "and I am sorry to give you this bad news today"
)
```

We can also collapse a vector of strings as followed:
```{r}
str_c(c("anemia", "low iron", "pallor"), collapse=" = ")
```

###  Subsetting strings

We can extract part of a string with `str_sub()`, and specify the inclusive positions (start and end) of the substring to extract. 

```{r}
list_of_strings <- c("dominant", "recessive", "X-linked", "mitochondrial")
str_sub(list_of_strings, 1, 4)
```

```{r}
str_sub(list_of_strings, -4, -1)
```

Even if you put "too big of a range", it will still works !
```{r}
str_sub(list_of_strings, 1, 15)
```

```{r}
str_sub(list_of_strings, -1, -1) <- str_to_upper(str_sub(list_of_strings, -1, -1))
list_of_strings
```

### Locales

Here are some functions to change the letter format:

```{r}
str_to_upper("recessive") # Change to upper case
str_to_lower("PIK3CA") # Change to lower case
str_to_title("MTOR pathway") # Capitalizes the first letter of each word
```

Locales are used to specify special characters according to the specific language. Different languages have different rules for changing case, hence the need to specify. ISO is a 639 language code, you can peak [here](http://www.loc.gov/standards/iso639-2/php/English_list.php).
```{r}
str_to_upper("i") # English
str_to_upper("i", "tr") # Turkish
```

Base R has some functions, like order() and sort() functions, which can sort strings using the current locale. But stringr functions is more consistent and flexible as it can take a locale argument. 
```{r}
list_of_strings <- c("recessive", "X-linked", "dominant",  "mitochondrial", "anticipation")
str_sort(list_of_strings, locale = "en")
str_sort(list_of_strings, locale = "haw")
```

```{r}
str_order(list_of_strings, locale = "en")
str_order(list_of_strings, locale = "haw")
```
Note. Apparently, the order can change according to the locale (language) selected, as showed [here](http://r4ds.had.co.nz/strings.html), but I must have tried a dozen languages and it never changed the order of my list. Oh well, I'll still keep it in mind just in case.

***

Exploring some functions.

*paste() and paste0()*
```{r}
paste("PIK3CA", "BRCA1", "TP53", "PTEN", "MLH1")
paste0("PIK3CA", "BRCA1", "TP53", "PTEN", "MLH1")
```
I found with this blog [here](https://www.r-bloggers.com/difference-between-paste-and-paste0/) that the difference between paste and paste0 is that their separator is " " and "" respectively.

*Get the character in the middle using str_length() and str_sub()*
```{r}
gene1 <- "BRCA1"
str_sub(gene1, ceiling(str_length(gene1)/2), ceiling(str_length(gene1)/2))
```

*Difference between sep and collapse?*

```{r}
str_c("BRCA1", "PTEN", sep = " and ") # Takes the list of strings as argument
str_c(c("BRCA1", "PTEN"), collapse = " + ") # Takes a vector as argument
```

The function str_wrap() can be used to reformat strings, but I see very little application for this function in my type of research work, so I will not illustrate it here.

str_trim() trim whitespace from start and enf of string.
```{r}
cancer_diagnoses <- str_trim(c(" breast cancer", "paraganglioma   ", "medullary thyroid cancer", "   glioblastoma", "pheochromocytoma"))
#str(cancer_diagnoses)
```

## Matching patterns with regular expressions

Find a substring to highlight with str_view()
```{r}
str_view(cancer_diagnoses, "oma")
```

A dot (".") can be used to replace any unknown character.
```{r}
str_view(cancer_diagnoses, "..oma")
```
Note. The two dots highlight the 2 characters preceeding "oma".

You have to use the escape `\\` in order to be able to search for the actual "." character in strings.
```{r}
dot <- "\\."
writeLines(dot)
str_view(c("abc", "a.c", "bef"), "a\\.c")
```

In order to look for a backslash, you need no less than 4 of them:
```{r}
x <- "a\\b"
writeLines(x)
#> a\b

str_view(x, "\\\\")
```

Why each of these strings don’t match a `\`: `"\"`, `"\\"`, `"\\\"` ?

* `"\"` : this doesn't work because the backslash is leading to an "escape" for the `"` and the expression is still open. 
* `"\\"`: this doesn't work because it literally prints `"\\"`.
* `"\\\"`: the expression is interpreted as is still open.

## Anchor

Here are some useful options as well: 

* ^ to match the start of the string.
* $ to match the end of the string.

```{r}
str_view(cancer_diagnoses, "oma$")
str_view(cancer_diagnoses, "^p")
```

```{r}
lung_cancer <- c("Non-Small Cell Lung Carcinoma", "Small Cell Lung Cancer", "Lung Carcinoid Tumor")
str_view(lung_cancer, "^Lung$") # Exact match is not found (starts with and finishes with), so it's not highlighted by this function
str_view(lung_cancer, "Lung") # Highlights all the words mathing "Lung" regardless of position of the word
```

### Replacing matched

Replace matches with new string (here, replace vowels by "*")
```{r}
str_replace(lung_cancer, "[aeiou]", "*")
str_replace_all(lung_cancer, "[aeiou]", "*")
```

Can be used for multiple replacements:
```{r}
copy_number <- c("2 copies of PTEN", "4 copies of GSK3B", "1 copy of TP53")
str_replace_all(copy_number, c("1" = "one", "2" = "two", "4" = "four"))
```

```{r}
sentences <- c("Lung cancer is largely due to tobacco smoking (active and passive).", "Sun exposure increases the risk for melanoma.", "Acute lymphoblastic leukemia is largely a diagnosis of the pediatric age group.")
sentences %>% 
  str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2") 
```
Note. This flips the order of the 2nd and 3rd word.

### Splitting

Let's explore str_split() function, which returns a list separated by a white space (as specified below)
```{r}
sentences %>%
  str_split(" ")
```

```{r}
"breast|brain|lung|pancreas" %>%
  str_split("\\|") %>%
  .[[1]]
```
Note. Since the vector length is one, it's easier to extract the 1st element of the list.

In a matrix, simplify = TRUE can be used:
```{r}
sentences %>%
  str_split(" ", simplify = TRUE)
```
Note. This puts the sentences in a "3 by 12" matrix, each cell containing 1 word (or nothing is the sentence is shorter than the longest sentence).
 
For example, we can isolate the word Japanese.  
```{r}
Japanese_disorders <- HumanDO$name[grep(x=HumanDO$name, pattern = "Japanese")]
Japanese_disorders %>% str_split(" ", n = 2, simplify = TRUE)
```
 
Highlights all the words or sentences using the boundary argument.
```{r}
str_view_all(Japanese_disorders, boundary("word")) 
str_view_all(Japanese_disorders, boundary("sentence")) 
```
This only takes the first element of the list.  
```{r}
str_split(Japanese_disorders, boundary("word"))[[1]]
```
 
### Find matches 

With regex() !

```{r}
encephalitis_disorders <- HumanDO$name[grep(x=HumanDO$name, pattern = "encephalitis")]
str_view(encephalitis_disorders, regex("vir"))
```
Note. This allows use to highligth words containing "vir", so here "viral" and "virus".

ignore_case = TRUE can be useful! 
```{r}
fever_disorders <- HumanDO$name[grep(x=HumanDO$name, pattern = "fever")]
str_view(fever_disorders, "rocky")
str_view(fever_disorders, regex("rocky", ignore_case = TRUE))
```
Note. In the first example, the word "Rocky" is not found because the first letter is capitalized, but using regex with ignore_case = TRUE resolves this issues.


Another useful one: multiline = TRUE allows ^ and $ to match the start and end of each line 
```{r}
x <- "BRCA1 1\nPTEN 2\nTP53 3\nBRCA2 4"
str_extract_all(x, "^BRCA")[[1]]
str_extract_all(x, regex("^BRCA", multiline = TRUE))[[1]]
```

As mentioned in the [tutorial](http://r4ds.had.co.nz/strings.html), comments = TRUE allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after #. To match a literal space, you’ll need to escape it: "\\ "

If we use genomic coordinates for example, with regex, you can put optional format so that the function can accept either a specific chromosomal location or a chromosomal region range.
```{r}
genomic_position <- regex("
  (^chr)   # specify the chromosome number will be indicated
  (\\d{1}) # chromosome number
  (:)      # nomenclature
  (\\d{7}) # five numbers
  [ -]?    # optional dash (for genomic range)
  (\\d{7})?
  ", comments = TRUE)
str_match("chr1:1234567", genomic_position)
str_match("chr1:1234567-3217654", genomic_position)
```

Note. dotall = TRUE allows . to match everything, including \n.
```{r}
genomic_position2 <- regex("
  (^chr)   # specify the chromosome number will be indicated
  (\\d{1}) # chromosome number
  (.)      # nomenclature
  (\\d{7}) # five numbers
  [-]?     # optional dash (for genomic range)
  (\\d{7})?
  ", comments = TRUE,
  dotall = TRUE)
str_match("chr1:1234567", genomic_position2)
str_match("chr1:1234567-3217654", genomic_position2)
str_match("chr1*1234567-3217654", genomic_position2)
```
Note. In the example above, I replace the ":" by "." and then regardless of the character at that position, the function will take it. 

Three other options for "regex-like" tasks.

Option 1 - fixed(), the fast one, ignores all special regular expressions.

The microbenchmark package is able to evaluate how much rime expressions are taking to run. 
```{r}
#install.packages("microbenchmark")
library(microbenchmark)
microbenchmark::microbenchmark(
  fixed = str_detect(sentences, fixed("the")),
  regex = str_detect(sentences, "the"),
  times = 20
)
```
Note.1. As you can see, the fixed() function takes about twice as much time to run than regex. 
Note.2. It can not be used with non-English data. As there are often multiple ways of representing the same character, this can cause problem. For example, there are two ways to define “á”: either as a single character or as an “a” plus an accent:
```{r}
a1 <- "\u00e1"
a2 <- "a\u0301"
c(a1, a2)
a1 == a2
```


Option 2 - coll()
```{r}
str_detect(a1, fixed(a2)) # Regex does not identify that the two terms are identical, because it looks at the litteral form and they are encoded differently
str_detect(a1, coll(a2)) # coll is able to identify it's the same letter 
```
Note.1. So I guess I won't be using French with regex then ;) 
Note.2. coll() though compares strings using standard collation rules, which is useful for doing case insensitive matching, and it takes a locale parameter.
```{r}
i <- c("I", "İ", "i", "ı")
i
str_subset(i, coll("i", ignore_case = TRUE)) # reads the Turkish "İ" as "I"
str_subset(i, coll("i", ignore_case = TRUE, locale = "tr")) # presents the Turkish "İ"
```

This allows you to identify your default locale (English Canada here).
```{r}
stringi::stri_locale_info()
```

The boundary argument used in str_split can also be used in the other stringr package functions. 
### Other uses of regular expressions

This function apropos() searches all objects available from the global environment, which is particularly helpful when you can't quite remember the object/function name but you know it contains a word like "replace".
```{r}
apropos("replace")
```
Note. I find it funny to see that the English language borrowed the term "apropos" from French. However, in the French language, this expression is actually written "à propos" and it can have two different meanings, as seen in the Collins dictionary [here](https://www.collinsdictionary.com/dictionary/french-english/%C3%A0-propos): "to show presence of mind, to do the right thing" or "suitably, aptly".

List all the files in the directory with dir()
```{r}
dir()
```

###  stringi

Talking of ontology, it appears that the stringi package is actually an ancestor of stringr, which was built on top of stringi. 

We can use the getNamespaceExports() to see what functions stringi has.
```{r}
library(stringi)
getNamespaceExports("stringi")
```


```{r}
stri_join("BRCA1", "BRCA2")
stri_compare("BRCA1", "BRCA2") # the only difference between these 2 words is a position -1
stri_info() # Get the default settings used by the ICU library.
```
Note. Here, ICU stands for the International Components for Unicode, which Wikipedia [here](https://en.wikipedia.org/wiki/International_Components_for_Unicode) tells me are a set of opend source C/C++ and Java libraries. However, my brain always thinks about Intensive Care Unit (ICU) when I read it because of my clinical training ;) 

#### Wow, this took a lot of time !! Of course, not all the lines have code and I tried something different, but still, reaching 600 lines for the first part of the homework, that is a a bit intense. 

***

# 5. Work with a list


```{r}

```

